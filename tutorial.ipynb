{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD\n",
    "\n",
    "In this lab, we introduce PyTorch basics, and how to use two deep music generation models designed by Music X Lab. Specifically,\n",
    "* **EC $^2$-VAE** for monophonic pitch contour and rhythm disentanglement. \n",
    "\n",
    "    *(Ruihan Yang et al., Deep Music Analogy Via Latent Representation Disentanglement)*\n",
    "    \n",
    "\n",
    "1. The data representation (input & output) for each model.\n",
    "2. Understand the model in a top-down order: starting from how to do style transfer using the example code, and looking into different models/modules while leaving the more detailed stuff as black boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pretty_midi as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./demo'):\n",
    "    os.mkdir('./demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: read through the entire project. Understand the code as much as you can in a top-down manner. Relate the code with the model diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part one: EC $^2$-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first preprare the trained model. We first initialize our model structure and then load its parameters.\n",
    "* The model structure is defined in the class `ec2vae.model.EC2VAE`. \n",
    "* The model parameter is saved in a `.pt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ec2vae.model import EC2VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "ec2vae_model = EC2VAE.init_model()\n",
    "\n",
    "# load model parameter\n",
    "ec2vae_param_path = './ec2vae/model_param/ec2vae-v1.pt'\n",
    "ec2vae_model.load_model(ec2vae_param_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's prepare some data and manipulate their latent codes. We use an array of length 32 to represent a 2-bar melody, where each time step corresponds to a 16-th note: 0-127 are MIDI pitches, 128 for sustain, and 130 for rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1: \"From the new world\" melody\n",
    "x1 = np.array([64, 128, 128, 67, 67, 128, 128, 128, 64, 128, 128, 62, 60, 128, 128, 128,\n",
    "               62, 128, 128, 64, 67, 128, 128, 64, 62, 128, 128, 128, 129, 129, 129, 129])\n",
    "\n",
    "# x2: C4, sixteenth notes.\n",
    "x2 = np.array([60] * 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to turn note arrays into one-hot vectors, i.e., piano-rolls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_array_to_onehot(note_array):\n",
    "    pr = np.zeros((len(note_array), 130))\n",
    "    pr[np.arange(0, len(note_array)), note_array.astype(int)] = 1.\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = note_array_to_onehot(x1)\n",
    "pr2 = note_array_to_onehot(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pr1, aspect='auto')\n",
    "plt.title('Display pr1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melody should be further converted to pytorch tensors, and to cuda/cpu. We should also unsqueeze a batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# to pytorch tensor\n",
    "pr1 = torch.from_numpy(pr1)\n",
    "\n",
    "# to float32\n",
    "pr1 = pr1.float()  \n",
    "\n",
    "# to device (if to cpu, the operation can be omitted.)\n",
    "pr1 = pr1.to(device)\n",
    "\n",
    "# unsqueeze the batch dim\n",
    "pr1 = pr1.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Concert pr2 similarly\n",
    "pr2 = torch.from_numpy(pr2).float().to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pr1.size(), pr2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the chords. In EC $^2$-VAE, we use 12-dim chord chroma representation. Chord is a time-series consisting of 32 tokens (16-th notes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful chords.\n",
    "amin = [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
    "gmaj = [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
    "fmaj = [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
    "emin = [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
    "cmaj = [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n",
    "cmin = [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1: Cmaj - - - | Gmaj - - - ||\n",
    "c1 = np.array([cmaj] * 16 + [gmaj] * 16)\n",
    "\n",
    "# c2: Amin - Gmaj - | Fmaj - Emin - ||\n",
    "c2 = np.array([amin] * 8 + [gmaj] * 8 + [fmaj] * 8 + [emin] * 8)\n",
    "\n",
    "# no chord\n",
    "c3 = np.zeros((32, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = torch.from_numpy(c1).float().to(device).unsqueeze(0)\n",
    "c2 = torch.from_numpy(c2).float().to(device).unsqueeze(0)\n",
    "c3 = torch.from_numpy(c3).float().to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to run model. Encode $z_p$ and $z_r$ by calling the encoder.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode melody 1 and chord C-G\n",
    "zp1, zr1 = ec2vae_model.encoder(pr1, c1)\n",
    "\n",
    "# encode melody 2 and \"no chord\"\n",
    "zp2, zr2 = ec2vae_model.encoder(pr2, c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zp1.size(), zr1.size(), zp2.size(), zr2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the reconstruction of `x_1` and do the what-if generation. We'll use `zp1` and its chord under 16-th note rhythm. We will also try a new chord progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_recon = ec2vae_model.decoder(zp1, zr1, c1)\n",
    "pred_new_rhythm = ec2vae_model.decoder(zp1, zr2, c1)\n",
    "pred_new_chord = ec2vae_model.decoder(zp1, zr1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be put back to cpu and to numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_recon = pred_recon.squeeze(0).cpu().numpy()\n",
    "out_new_rhythm = pred_new_rhythm.squeeze(0).cpu().numpy()\n",
    "out_new_chord = pred_new_chord.squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_new_rhythm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the generation to MIDI files. The following function converts note array to a list of pretty_midi Notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_recon = ec2vae_model.__class__.note_array_to_notes(out_recon, bpm=120, start=0.)\n",
    "notes_new_rhythm = ec2vae_model.__class__.note_array_to_notes(out_new_rhythm, bpm=120, start=0.)\n",
    "notes_new_chord = ec2vae_model.__class__.note_array_to_notes(out_new_chord, bpm=120, start=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function coverts chord to a list of pretty_midi notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_c1 = ec2vae_model.__class__.chord_to_notes(c1.squeeze(0).cpu().numpy(), 120, 0)\n",
    "notes_c2 = ec2vae_model.__class__.chord_to_notes(c2.squeeze(0).cpu().numpy(), 120, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate three MIDI files. Note:\n",
    "1. The original \"From the new world\" melody should be played with `c1`.\n",
    "2. The melody transferred to 16-th note rhythm should also be played with `c1`.\n",
    "3. The melody transferred to a new chord progression should be played with `c2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_midi_with_melody_chord(fn, mel_notes, c_notes):\n",
    "    midi = pm.PrettyMIDI()\n",
    "    ins1 = pm.Instrument(0)\n",
    "    ins1.notes = mel_notes\n",
    "    ins2 = pm.Instrument(0)\n",
    "    ins2.notes = c_notes\n",
    "    midi.instruments.append(ins1)\n",
    "    midi.instruments.append(ins2)\n",
    "    midi.write(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_midi_with_melody_chord('./demo/ec2vae-recon.mid', notes_recon, notes_c1)\n",
    "generate_midi_with_melody_chord('./demo/ec2vae-new-rhythm.mid', notes_new_rhythm, notes_c1)\n",
    "generate_midi_with_melody_chord('./demo/ec2vae-new-chord.mid', notes_new_chord, notes_c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "1. Write a new melody (maybe with new chord progression). Try transferring the original melody to the new melody contour. During encoding and decode, which chord should we use as condition?\n",
    "2. Change chord and use same zp and zr. Check the controllability from chord condition. (Our model should not perform very well. Test it on your own!)\n",
    "3. Consider longer melody, change it per 2-bar.\n",
    "4. More to explore: sampling from the prior or posterior. (Hint: to get the posterior distribution, re-write the encoder function output.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
